{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download a public dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big><b>File completed</b></big> (05/12/2021)  \n",
    "Table of Contents + tool versions on 06/22/2021\n",
    "\n",
    "Bénédicte Noblet\n",
    "\n",
    "- Preparing session on IFB core cluster\n",
    "- Online researches for a dataset\n",
    "- Download data files with SRA Toolkit (*sra-tools*)\n",
    "- Files and folders summary when leaving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <b>Preparing session on IFB core cluster</b>\n",
    "\n",
    "<em>loaded JupyterLab</em> : Version 2.2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cell launched on Tue May 11 17:18:52 CEST 2021 ===\n",
      "=== Current IFB session size: Medium (4CPU, 10GB) or Large (10CPU, 50GB) ===\n",
      "       JobID  AllocCPUS        NodeList \n",
      "------------ ---------- --------------- \n",
      "16367476             10     cpu-node-47 \n",
      "16367476.ba+         10     cpu-node-47 \n",
      "16367476.0           10     cpu-node-47 \n",
      "=== Working's root folder is ===\n",
      "/shared/projects/gonseq/Building/\n",
      "\n",
      "=== current folder tree ===\n",
      "/shared/projects/gonseq/Building/\n",
      "├── Data\n",
      "│   └── info\n",
      "│       ├── 16samples_SraRunTable.txt\n",
      "│       ├── 16samples_SRR_Acc_List.txt\n",
      "│       ├── SraRunTable.txt\n",
      "│       └── SRR_Acc_List.txt\n",
      "├── Pipeline\n",
      "│   └── Pipe_1-bash_download-files.ipynb\n",
      "└── Results\n",
      "\n",
      "4 directories, 5 files\n",
      "=== current working directory ===\n",
      "/shared/ifbstor1/projects/gonseq/Building/Pipeline\n"
     ]
    }
   ],
   "source": [
    "echo \"=== Cell launched on $(date) ===\"\n",
    "\n",
    "echo \"=== Current IFB session size: Medium (4CPU, 10GB) or Large (10CPU, 50GB) ===\"\n",
    "jobid=$(squeue -hu $USER | awk '/jupyter/ {print $1}')\n",
    "sacct --format=JobID,AllocCPUS,NODELIST -j ${jobid}\n",
    "\n",
    "echo \"=== Working's root folder is ===\"\n",
    "gohome=\"/shared/projects/gonseq/Building/\"\n",
    "echo \"${gohome}\"\n",
    "echo \"\"\n",
    "\n",
    "echo \"=== current folder tree ===\"\n",
    "tree \"${gohome}\"\n",
    "echo \"=== current working directory ===\"\n",
    "echo \"${PWD}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If your datatest is already available on the server, skip this part... Unless you want to know how to do for next time ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>I- Online researches</b>\n",
    "\n",
    "Several cases can happen:\n",
    "- you read a wonderful paper and want to analyze the dataset they use <br>\n",
    "- you have a project but miss the data (and sequencing platform !) to do it <br>\n",
    "- a enthusiastic PI ask you a question you don't yet have the answer <br>\n",
    "\n",
    "For the two last options, you need to find a dataset before going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2- Find a paper dataset** <a id=\"section005\"></a>\n",
    "\n",
    "To that purpose, we can browse <a href=\"https://www.ncbi.nlm.nih.gov/gds\"> <b>GEO datasets</b> </a> with some filters. <br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    There is an alternative to the amercian NCBI database in Europe: <a href=\"https://www.ebi.ac.uk/ena/browser/home\">ENA (<i>European Nucleotide Archive</i>)</a> from <a href=\"https://www.ebi.ac.uk/\">EMBL-EBI</a> institute and part of the <a href=\"https://elixir-europe.org/platforms/data/core-data-resources\">Elixir Core Data resource</a>.<br>\n",
    "    Unfortunately, to the authors' knowledge, there is not any command-line tool to retrieve datafiles when a dataset is identified. So, we will use NCBI's resources for now.\n",
    "</div>\n",
    "\n",
    "Starting on the main page, proceed to a research with some (2 to 3, including RNAseq) key words about the research project you have. <br>\n",
    "Once you face the huge amount of results, add following filters (left side bar of webpage) :\n",
    "- *Organism* > *Customize* > *Browse* (Homo sapiens, Mus musculus, Rattus Norvegicus, ...) > select those you want to display > *Show*. Then click on the one or those you want\n",
    "- *Study type* > Select only *Expression Profiling by high throughput sequencing*. Back on the results page, click on the added option to set it active\n",
    "- If you still have too many options to handle, select *Tissue* in the *Attribute name* option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then browse the results you have to find a paper that suits your project. <br>\n",
    "When you pick up one, keep record of some access references:\n",
    "<blockquote>\n",
    "GEO Series Number: GSExxxxxx <br>\n",
    "SRA related Number: SRPxxxxxx <br>\n",
    "SRP Run experiment: SRXxxxxxxx <br>\n",
    "BioProject reference: PRNJAxxxxxx\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3- Retrieve dataset informations**\n",
    "\n",
    "Go to the <a href='https://www.ncbi.nlm.nih.gov/Traces/study/'>SRA (Sequence Read Archive) Run Selector</a> portal from the NCBI.\n",
    "> Another option is the <a href='https://trace.ncbi.nlm.nih.gov/Traces/sra/'>SRA</a> website itself but it seems deprecated. The NCBI probably made an update last year (2020) as many other institutes did.\n",
    "\n",
    "Using the search bar with your SRA accession number, open the page of the dataset you choose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down the page, you can select de samples you want amoung the full list. <br>\n",
    "Whether you select samples or want to take the entire dataset, download the `Metadata`and the `Accession List` from the `Select` section in the middle of the page.\n",
    "> Add an accession reference at the beginning of the file names (`SraRunTable.txt` and `SRR_Acc_List.txt` respectively) to distinguish series when you saved the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <b>II- Download data files with SRA Toolkit (<i>sra-tools</i>)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively to use of sra command lines, we could retrieve the list of lines to download fastq files from [sra-explorer web interface](https://ewels.github.io/sra-explorer/) developped by Phil Ewels <a href=\"https://www.biostars.org/p/366721/#366722\"><i>as a fun little side project</i></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1- Tools versions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load needed tools and display their current versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== sra-tools modules =====\n",
      "\n",
      "\"prefetch\" version 2.10.3\n",
      "\n",
      "\n",
      "vdb-validate : 2.10.3\n",
      "\n",
      "\n",
      "\"fasterq-dump\" version 2.10.3\n",
      "\n",
      "===== compression tool =====\n",
      "pigz 2.3.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "module load sra-tools pigz\n",
    "# module load sra-tools/2.10.3 pigz/2.3.4\n",
    "\n",
    "echo \"===== sra-tools modules =====\"\n",
    "prefetch --version\n",
    "vdb-validate --version\n",
    "fasterq-dump --version\n",
    "echo \"===== compression tool =====\"\n",
    "pigz --version\n",
    "echo \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2- Download SRA files with accession list**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul class=\"alert alert-block alert-info\">\n",
    "    <li>\n",
    "        about sra toolkit <a href=\"https://www.ncbi.nlm.nih.gov/sra/docs/sradownload/\">previous</a> and <a href=\"https://hpc.nih.gov/apps/sratoolkit.html\">latest releases</a>\n",
    "    </li>\n",
    "    <li>\n",
    "    another available option is to use <a href=\"https://www.biostars.org/p/366721/#366722\">sra-explorer web interface</a> to retrieve <code>wget</code> or <code>curl</code> bash command lines\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "We will dowload the ``.sra`` files (compressed ones) using the accession list we have downloaded before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accesslist=\"${gohome}Data/info/16samples_SRR_Acc_List.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call this file ``accesslist`` for now on.  \n",
    "\n",
    "To see inside and know which references will be fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRR7430744\n",
      "SRR7430741\n",
      "SRR7430706\n",
      "SRR7430707\n",
      "SRR7430708\n",
      "SRR7430709\n",
      "SRR7430710\n",
      "SRR7430711\n",
      "SRR7430712\n",
      "SRR7430713\n",
      "SRR7430739\n",
      "SRR7430740\n",
      "SRR7430738\n",
      "SRR7430742\n",
      "SRR7430743\n",
      "SRR7430745"
     ]
    }
   ],
   "source": [
    "cat \"${accesslist}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store downloaded files in ``Data/`` in a subfolder called ``sra``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p \"${gohome}Data/sra/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``prefetch`` is verbose, we can see dates and how it goes directly into the notebook output. As it will take some time, we will eventually disconnect and loose tracks. To prevent information lost, we <a href=\"https://www.cyberciti.biz/faq/how-to-write-the-output-into-the-file-in-linux/\">export</a> and write screen lines in a file (here called <code>logfile</code>).\n",
    "<blockquote>\n",
    "    <code>&#38;>></code> to capture both standard output and error output and append a file <br>\n",
    "    <code>|&#38;</code> to capture both standard output and error output to append a file, still diplaying information on screen <br>\n",
    "    <code>tee</code> read from standard input and write to standard output and files <br>\n",
    "    <code>-a</code> for <code>tee</code> command to append file instead of overwriting it\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to launch <code>prefetch</code> tool with this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screen output is redirected to /shared/projects/gonseq/Building/Data/sra-files_retrieval.log.\n",
      "=== starting for SRR7430744...\n",
      "... done Tue May 11 17:21:01 CEST 2021 ===\n",
      "=== starting for SRR7430741...\n",
      "... done Tue May 11 17:24:09 CEST 2021 ===\n",
      "=== starting for SRR7430706...\n",
      "... done Tue May 11 17:25:39 CEST 2021 ===\n",
      "=== starting for SRR7430707...\n",
      "... done Tue May 11 17:27:06 CEST 2021 ===\n",
      "=== starting for SRR7430708...\n",
      "... done Tue May 11 17:28:56 CEST 2021 ===\n",
      "=== starting for SRR7430709...\n",
      "... done Tue May 11 17:30:17 CEST 2021 ===\n",
      "=== starting for SRR7430710...\n",
      "... done Tue May 11 17:31:53 CEST 2021 ===\n",
      "=== starting for SRR7430711...\n",
      "... done Tue May 11 17:33:37 CEST 2021 ===\n",
      "=== starting for SRR7430712...\n",
      "... done Tue May 11 17:35:07 CEST 2021 ===\n",
      "=== starting for SRR7430713...\n",
      "... done Tue May 11 17:36:38 CEST 2021 ===\n",
      "=== starting for SRR7430739...\n",
      "... done Tue May 11 17:38:08 CEST 2021 ===\n",
      "=== starting for SRR7430740...\n",
      "... done Tue May 11 17:39:50 CEST 2021 ===\n",
      "=== starting for SRR7430738...\n",
      "... done Tue May 11 17:41:22 CEST 2021 ===\n",
      "=== starting for SRR7430742...\n",
      "... done Tue May 11 17:42:45 CEST 2021 ===\n",
      "=== starting for SRR7430743...\n",
      "... done Tue May 11 17:44:52 CEST 2021 ===\n",
      "=== starting for SRR7430745...\n",
      "... done Tue May 11 17:47:26 CEST 2021 ===\n",
      "\n",
      "real\t27m40.221s\n",
      "user\t6m8.621s\n",
      "sys\t2m38.554s\n"
     ]
    }
   ],
   "source": [
    "logfile=\"${gohome}Data/sra-files_retrieval.log\"\n",
    "echo \"Screen output is redirected to ${logfile}.\"\n",
    "\n",
    "# as time command does not redirect output\n",
    "echo \"operation starting by $(date)\" >> ${logfile}\n",
    "\n",
    "time for srrnum in $(cat \"${accesslist}\"); do\n",
    "\n",
    "    echo \"=== starting for ${srrnum}...\" |& tee -a ${logfile}\n",
    "    date &>> ${logfile}\n",
    "\n",
    "    prefetch -O \"${gohome}Data/sra/\" \\\n",
    "             ${srrnum} \\\n",
    "             &>> ${logfile}\n",
    "\n",
    "    echo \"... done $(date) ===\" |& tee -a ${logfile}\n",
    "\n",
    "done\n",
    "\n",
    "echo \"operation finished by $(date)\" >> ${logfile}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``prefetch`` options we use or you can add:\n",
    "<blockquote>\n",
    "    <code>-O</code> to specify destination folder <br>\n",
    "    <code>-t</code> to specify where the temporary files are to be written  <br>\n",
    "    <code>-c</code> to check if presents else download file (default, look into current working directory) <br>\n",
    "    <code>-f</code> to force overwrite existing files instead of fail <br>\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we have all expected files before going on.  \n",
    "``prefetch`` tool put every ``.sra`` file in a folder named by its accession number, so we will use the ``tree`` command to visualise them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 directories, 16 files\n"
     ]
    }
   ],
   "source": [
    "tree \"${gohome}Data/sra/\" >> ${logfile}\n",
    "tree \"${gohome}Data/sra/\" | grep \"files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have same numbers between samples references and downloaded file, it's a good beginning point. Let's check there is no issue with those files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3- Check sra files integrity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the tool ``vdb-validate`` from ``sra-tools`` to know if files matches those on web archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screen output is redirected to /shared/projects/gonseq/Building/Data/sra-files_integrity.log\n",
      "... working on SRR7430706.sra file...\n",
      "... working on SRR7430707.sra file...\n",
      "... working on SRR7430708.sra file...\n",
      "... working on SRR7430709.sra file...\n",
      "... working on SRR7430710.sra file...\n",
      "... working on SRR7430711.sra file...\n",
      "... working on SRR7430712.sra file...\n",
      "... working on SRR7430713.sra file...\n",
      "... working on SRR7430738.sra file...\n",
      "... working on SRR7430739.sra file...\n",
      "... working on SRR7430740.sra file...\n",
      "... working on SRR7430741.sra file...\n",
      "... working on SRR7430742.sra file...\n",
      "... working on SRR7430743.sra file...\n",
      "... working on SRR7430744.sra file...\n",
      "... working on SRR7430745.sra file...\n",
      "\n",
      "real\t1m4.760s\n",
      "user\t0m42.092s\n",
      "sys\t0m16.848s\n"
     ]
    }
   ],
   "source": [
    "logfile=\"${gohome}Data/sra-files_integrity.log\"\n",
    "echo \"Screen output is redirected to ${logfile}\"\n",
    "\n",
    "echo \"operation starting by $(date)\" >> ${logfile}\n",
    "\n",
    "time for srrfile in $(find \"${gohome}\" -name \"*.sra\" | sort); do\n",
    "\n",
    "    echo \"... working on $(basename ${srrfile}) file...\" |& tee -a ${logfile}\n",
    "    vdb-validate \"${srrfile}\" &>> ${logfile}\n",
    "                 \n",
    "done\n",
    "\n",
    "echo \"operation finished by $(date)\" >> ${logfile}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can either open ``.log`` file within a text editor (left panel, browse for file and right click on it) or display it in this notebook to read.  \n",
    "A shorter option consists in showing only lines that concern ``.sra`` files (extension with ``.`` and ``' `` either we will have comment lines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-12T09:49:28 vdb-validate.2.10.3 info: Table 'SRR7430706.sra' metadata: md5 ok\n",
      "2021-05-12T09:49:32 vdb-validate.2.10.3 info: Table 'SRR7430706.sra' is consistent\n",
      "2021-05-12T09:49:32 vdb-validate.2.10.3 info: Table 'SRR7430707.sra' metadata: md5 ok\n",
      "2021-05-12T09:49:36 vdb-validate.2.10.3 info: Table 'SRR7430707.sra' is consistent\n",
      "2021-05-12T09:49:36 vdb-validate.2.10.3 info: Table 'SRR7430708.sra' metadata: md5 ok\n",
      "2021-05-12T09:49:40 vdb-validate.2.10.3 info: Table 'SRR7430708.sra' is consistent\n",
      "2021-05-12T09:49:40 vdb-validate.2.10.3 info: Table 'SRR7430709.sra' metadata: md5 ok\n",
      "2021-05-12T09:49:44 vdb-validate.2.10.3 info: Table 'SRR7430709.sra' is consistent\n",
      "2021-05-12T09:49:44 vdb-validate.2.10.3 info: Table 'SRR7430710.sra' metadata: md5 ok\n",
      "2021-05-12T09:49:49 vdb-validate.2.10.3 info: Table 'SRR7430710.sra' is consistent\n",
      "2021-05-12T09:49:49 vdb-validate.2.10.3 info: Table 'SRR7430711.sra' metadata: md5 ok\n",
      "2021-05-12T09:49:52 vdb-validate.2.10.3 info: Table 'SRR7430711.sra' is consistent\n",
      "2021-05-12T09:49:53 vdb-validate.2.10.3 info: Table 'SRR7430712.sra' metadata: md5 ok\n",
      "2021-05-12T09:49:56 vdb-validate.2.10.3 info: Table 'SRR7430712.sra' is consistent\n",
      "2021-05-12T09:49:56 vdb-validate.2.10.3 info: Table 'SRR7430713.sra' metadata: md5 ok\n",
      "2021-05-12T09:50:00 vdb-validate.2.10.3 info: Table 'SRR7430713.sra' is consistent\n",
      "2021-05-12T09:50:00 vdb-validate.2.10.3 info: Table 'SRR7430738.sra' metadata: md5 ok\n",
      "2021-05-12T09:50:04 vdb-validate.2.10.3 info: Table 'SRR7430738.sra' is consistent\n",
      "2021-05-12T09:50:04 vdb-validate.2.10.3 info: Table 'SRR7430739.sra' metadata: md5 ok\n",
      "2021-05-12T09:50:08 vdb-validate.2.10.3 info: Table 'SRR7430739.sra' is consistent\n",
      "2021-05-12T09:50:08 vdb-validate.2.10.3 info: Table 'SRR7430740.sra' metadata: md5 ok\n",
      "2021-05-12T09:50:12 vdb-validate.2.10.3 info: Table 'SRR7430740.sra' is consistent\n",
      "2021-05-12T09:50:12 vdb-validate.2.10.3 info: Table 'SRR7430741.sra' metadata: md5 ok\n",
      "2021-05-12T09:50:15 vdb-validate.2.10.3 info: Table 'SRR7430741.sra' is consistent\n",
      "2021-05-12T09:50:15 vdb-validate.2.10.3 info: Table 'SRR7430742.sra' metadata: md5 ok\n",
      "2021-05-12T09:50:19 vdb-validate.2.10.3 info: Table 'SRR7430742.sra' is consistent\n",
      "2021-05-12T09:50:19 vdb-validate.2.10.3 info: Table 'SRR7430743.sra' metadata: md5 ok\n",
      "2021-05-12T09:50:22 vdb-validate.2.10.3 info: Table 'SRR7430743.sra' is consistent\n",
      "2021-05-12T09:50:23 vdb-validate.2.10.3 info: Table 'SRR7430744.sra' metadata: md5 ok\n",
      "2021-05-12T09:50:26 vdb-validate.2.10.3 info: Table 'SRR7430744.sra' is consistent\n",
      "2021-05-12T09:50:26 vdb-validate.2.10.3 info: Table 'SRR7430745.sra' metadata: md5 ok\n",
      "2021-05-12T09:50:33 vdb-validate.2.10.3 info: Table 'SRR7430745.sra' is consistent\n"
     ]
    }
   ],
   "source": [
    "cat ${logfile} | grep \".sra' \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to count for lines that contains ***ok*** (each sample with 1 for ``md5`` and 5 for ``checksums``) or ***is consistent*** (1 per ``.sra`` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "cat ${logfile} | grep \"md5 ok\" | wc -l\n",
    "echo $(( $(cat ${logfile} | grep \"checksums ok\" | wc -l) / 5))\n",
    "cat ${logfile} | grep \"is consistent\" | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4- ``.fastq.gz`` files generation: introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul class=\"alert alert-block alert-info\">\n",
    "    <li>\n",
    "        about <a href=\"https://github.com/ncbi/sra-tools/wiki/08.-prefetch-and-fasterq-dump\">prefetch and fasterq-dump</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"https://usegalaxy.eu/\">usegalaxy.eu</a>: first section (<i>Get Data</i>), <br>workflow called <b>Faster Download and Extract Reads in FASTQ format from NCBI SRA</b>\n",
    "    </li>\n",
    "    <li>\n",
    "        about <a href=\"https://zlib.net/pigz/pigz.pdf\">pigz</a> for multithreading\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``.sra`` files downloaded in the previous cells are 'small' files compared to ``.fastq`` files. The idea is to download small amounts of data to avoid network issues during the process... but we can't start our analyses on them. We have to re-create raw ``.fastq`` files first. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To that purpose, ``fasterq-dump`` tool (derived from ``fastq-dump`` in previous sra-tools releases) works on a ``.sra`` file and decompress it in a fastq file (default, 1 per read so 2 for pair end data, PE). <br>\n",
    "\n",
    "The default command line is: ``fasterq-dump filename.sra``.   \n",
    "It performs reads separation in 3 files (``--split-3`` option in ``fastq-dump`` tool) for paired-end data: 1 file per read direction for mate reads and another one for remaining unmated reads.  \n",
    "We can add some options to specify folders to be used, otherwise generated files will be written in current working directory. <br>\n",
    "<blockquote>\n",
    "    <code>--outdir</code>, to know where to find the <code>.fastq</code> files <br>\n",
    "    <code>--temp</code>, to control where temporary files are, indeed temporarily, stored (and check they are removed after :-) ) <br>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As ``.fastq`` files are quite big files, we'll add the compression step right after any ``.fastq`` files is created. The tool, called ``pigz``, compresses files in the ``.gzip`` format that can be handled downstream. <br>\n",
    "This tool is based on ``gzip`` tool (so an anagram or an acronym for *Pigz is gzip*?). It allows to have several processes in parallel to work on a the same file... and it goes faster than ``gzip`` tool, even if it still takes quite some time: up to 8-9 minutes for the biggest file among the 16 selected in ``gonseq`` (SRR7430745). <br>\n",
    "<blockquote>\n",
    "    <code>-p number</code> or <code>--processes number</code> to allow up to <code>number</code> compression threads (default is the number of online processors, or 8 if unknown)\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5- To obtain ``.fastq.gz`` files from ``.sra``**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ``--outdir`` in ``fasterq-dump`` command, we will place created files in following folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfolder=\"${gohome}Data/fastq/raw/\"\n",
    "mkdir -p \"${outfolder}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be patient while running following *big loop*'s cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screen output is redirected to /shared/projects/gonseq/Building/Data/sra-files_creation_fastqgz.log.\n",
      "========== Processing sample SRR7430706 ===========\n",
      "... compressing SRR7430706_1.fastq...\n",
      "... compressing SRR7430706_2.fastq...\n",
      "========== Processing sample SRR7430707 ===========\n",
      "... compressing SRR7430707_1.fastq...\n",
      "... compressing SRR7430707_2.fastq...\n",
      "========== Processing sample SRR7430708 ===========\n",
      "... compressing SRR7430708_1.fastq...\n",
      "... compressing SRR7430708_2.fastq...\n",
      "========== Processing sample SRR7430709 ===========\n",
      "... compressing SRR7430709_1.fastq...\n",
      "... compressing SRR7430709_2.fastq...\n",
      "========== Processing sample SRR7430710 ===========\n",
      "... compressing SRR7430710_1.fastq...\n",
      "... compressing SRR7430710_2.fastq...\n",
      "========== Processing sample SRR7430711 ===========\n",
      "... compressing SRR7430711_1.fastq...\n",
      "... compressing SRR7430711_2.fastq...\n",
      "========== Processing sample SRR7430712 ===========\n",
      "... compressing SRR7430712_1.fastq...\n",
      "... compressing SRR7430712_2.fastq...\n",
      "========== Processing sample SRR7430713 ===========\n",
      "... compressing SRR7430713_1.fastq...\n",
      "... compressing SRR7430713_2.fastq...\n",
      "========== Processing sample SRR7430738 ===========\n",
      "... compressing SRR7430738_1.fastq...\n",
      "... compressing SRR7430738_2.fastq...\n",
      "========== Processing sample SRR7430739 ===========\n",
      "... compressing SRR7430739_1.fastq...\n",
      "... compressing SRR7430739_2.fastq...\n",
      "========== Processing sample SRR7430740 ===========\n",
      "... compressing SRR7430740_1.fastq...\n",
      "... compressing SRR7430740_2.fastq...\n",
      "========== Processing sample SRR7430741 ===========\n",
      "... compressing SRR7430741_1.fastq...\n",
      "... compressing SRR7430741_2.fastq...\n",
      "========== Processing sample SRR7430742 ===========\n",
      "... compressing SRR7430742_1.fastq...\n",
      "... compressing SRR7430742_2.fastq...\n",
      "========== Processing sample SRR7430743 ===========\n",
      "... compressing SRR7430743_1.fastq...\n",
      "... compressing SRR7430743_2.fastq...\n",
      "========== Processing sample SRR7430744 ===========\n",
      "... compressing SRR7430744_1.fastq...\n",
      "... compressing SRR7430744_2.fastq...\n",
      "========== Processing sample SRR7430745 ===========\n",
      "... compressing SRR7430745_1.fastq...\n",
      "... compressing SRR7430745_2.fastq...\n",
      "\n",
      "real\t118m38.588s\n",
      "user\t900m28.389s\n",
      "sys\t36m46.251s\n",
      "Step done\n"
     ]
    }
   ],
   "source": [
    "logfile=\"${gohome}Data/sra-files_creation_fastqgz.log\"\n",
    "echo \"Screen output is redirected to ${logfile}.\"\n",
    "\n",
    "# as time command does not redirect output\n",
    "echo \"operation starting by $(date)\" >> ${logfile}\n",
    "\n",
    "time for srrnum in $(ls \"${gohome}Data/sra/\"); do\n",
    "    \n",
    "    echo \"========== Processing sample ${srrnum} ===========\" |& tee -a ${logfile}\n",
    "    srrfolder=\"${gohome}Data/sra/${srrnum}\"\n",
    "    fasterq-dump --outdir ${outfolder} \\\n",
    "                 --temp ${outfolder} \\\n",
    "                 ${srrfolder} \\\n",
    "                 &>> ${logfile}\n",
    "    \n",
    "    # don't assume there are 1 or 2 fastq by sample (3rd one if unmated reads)\n",
    "    for fullfile in $(ls \"${outfolder}\"*.fastq); do\n",
    "    \n",
    "        echo \"... compressing $(basename ${fullfile})...\" |& tee -a ${logfile}\n",
    "        date >> ${logfile}\n",
    "        pigz --processes 9 ${fullfile}\n",
    "        date >> ${logfile}\n",
    "    done\n",
    "    \n",
    "done\n",
    "\n",
    "echo \"operation finished by $(date)\" >> ${logfile}\n",
    "ls -lh \"${outfolder}\" >> ${logfile}\n",
    "echo \"Step done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>III- Files and folders summary when leaving</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of files in our destination folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "ls \"${outfolder}\" | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and check the arborescence of our folder with the Unix command `tree`.  \n",
    "<blockquote>\n",
    "    Adding <code>-L</code> option and a number allows to stop digging into the tree folder... and let the output be still readable. <br>\n",
    "    To list only directories, use option <code>-d</code>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/shared/projects/gonseq/Building/\n",
      "├── Data\n",
      "│   ├── fastq\n",
      "│   │   └── raw\n",
      "│   ├── info\n",
      "│   └── sra\n",
      "│       ├── SRR7430706\n",
      "│       ├── SRR7430707\n",
      "│       ├── SRR7430708\n",
      "│       ├── SRR7430709\n",
      "│       ├── SRR7430710\n",
      "│       ├── SRR7430711\n",
      "│       ├── SRR7430712\n",
      "│       ├── SRR7430713\n",
      "│       ├── SRR7430738\n",
      "│       ├── SRR7430739\n",
      "│       ├── SRR7430740\n",
      "│       ├── SRR7430741\n",
      "│       ├── SRR7430742\n",
      "│       ├── SRR7430743\n",
      "│       ├── SRR7430744\n",
      "│       └── SRR7430745\n",
      "├── Pipeline\n",
      "└── Results\n",
      "\n",
      "23 directories\n"
     ]
    }
   ],
   "source": [
    "tree -d -L 3 \"${gohome}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know how many disk space files take (either shortest output or detailed one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75G\t/shared/projects/gonseq/Building/Data/fastq/raw/\n",
      "75G\ttotal\n"
     ]
    }
   ],
   "source": [
    "du -ch -d1 \"${outfolder}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 75G\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.4G May 12 14:35 SRR7430706_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.5G May 12 14:35 SRR7430706_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.1G May 12 14:43 SRR7430707_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.1G May 12 14:43 SRR7430707_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.5G May 12 14:50 SRR7430708_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.5G May 12 14:50 SRR7430708_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.2G May 12 14:58 SRR7430709_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.3G May 12 14:58 SRR7430709_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.6G May 12 15:05 SRR7430710_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.7G May 12 15:05 SRR7430710_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.3G May 12 15:14 SRR7430711_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.3G May 12 15:14 SRR7430711_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.0G May 12 15:21 SRR7430712_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.0G May 12 15:21 SRR7430712_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.4G May 12 15:28 SRR7430713_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.5G May 12 15:28 SRR7430713_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.5G May 12 15:36 SRR7430738_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.6G May 12 15:36 SRR7430738_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.1G May 12 15:43 SRR7430739_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.2G May 12 15:43 SRR7430739_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.3G May 12 15:50 SRR7430740_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.4G May 12 15:50 SRR7430740_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 1.8G May 12 15:57 SRR7430741_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 1.8G May 12 15:57 SRR7430741_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.2G May 12 16:03 SRR7430742_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.2G May 12 16:03 SRR7430742_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.0G May 12 16:09 SRR7430743_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 2.1G May 12 16:09 SRR7430743_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 1.9G May 12 16:15 SRR7430744_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 1.9G May 12 16:15 SRR7430744_2.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 4.0G May 12 16:23 SRR7430745_1.fastq.gz\n",
      "-rw-rw----+ 1 bnoblet bnoblet 4.2G May 12 16:23 SRR7430745_2.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "ls -lh \"${outfolder}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "___\n",
    "\n",
    "Now we can go on to get an eye on data and check samples quality.  \n",
    "  \n",
    "**=> Lecture 2 : Raw data quality** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
